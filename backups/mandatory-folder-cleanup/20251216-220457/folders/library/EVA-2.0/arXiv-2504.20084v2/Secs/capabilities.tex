\section{AI Awareness and AI Capabilities}
\label{sec:opportunity}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/capa_and_risk.png} 
    \caption{Mapping between capabilities, awareness dimensions, and risks. Each awareness type connects relevant system capabilities with corresponding safety risks}
    \label{fig:capa_and_risk}
\end{figure}

In this section, we explore the relationship between AI awareness and its observable capabilities\footnote{We use the word \emph{``observable''} since, like humans, we believe that cognitive-level awareness is more fundamental than externally observable behaviors. In modern cognitive science, awareness is widely understood as a deeper, guiding layer that actively regulates and shapes behavior, rather than merely reflecting it \citep{baars1993cognitive, koriat2006metacognition, dehaene2011experimental}.}. We primarily focus on two key aspects of AI capabilities: (1) reasoning and autonomous planning, and (2) safety and trustworthiness, with brief discussions of other relevant capabilities. Our goal is to provide a deeper understanding of how these factors reflect and interact with the capabilities of modern AI systems.




\subsection{Reasoning and Autonomous Planning}

Reasoning and autonomous task planning have been foundational objectives of AI research since its inception \citep{ghallab2004automated}.
In complex, multi-step problem-solving scenarios, an AI agent must perform deep reasoning while autonomously planning tasks. To achieve this, two key forms of AI awareness are often engaged: metacognition and situational awareness. Metacognition enables the model to monitor and regulate its own thinking processes, while situational awareness helps it understand external constraints and the context of the task. 

\paragraph{Self‑Correction} 

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/capa/self-correct.pdf} 
    \caption{Illustration of Reflexion’s self-correction cycle driven by metacognitive reflection across tasks (\citet{shinn2023reflexion}). After generating an initial response, the agent receives internal or external feedback (\eg, from unit test failures, heuristic rules, or reasoning errors) and employs metacognitive reasoning to explicitly reflect on and rectify its mistakes. Examples from decision-making, programming, and reasoning tasks demonstrate how metacognitive self-monitoring enhances accuracy and efficiency}
    \label{fig:self_correct}
\end{figure}

Self‑correction leverages metacognitive loops to identify and rectify reasoning errors during generation. Reflexion augments chain‑of‑thought (CoT) \citep{wei2022chain} with a feedback loop: after an initial answer, the model reflects on its own output, generates critiques, and then refines the solution, leading to substantial gains in benchmark performance \citep{shinn2023reflexion} (see \autoref{fig:self_correct}). Similarly, Self‑Consistency samples multiple reasoning paths and aggregates them to mitigate individual path errors, effectively performing an implicit self‑check \citep{wangself}. These techniques demonstrate that embedding self‑monitoring directly into the generation process can improve model performance.
However, intrinsic self‑correction (\ie, only self-monitoring is engaged without external, oracle feedback) remains notoriously unstable: \citet{huanglarge} show that without external feedback or oracle labels, LLMs often fail to improve\textemdash and can even degrade\textemdash in reasoning tasks after self‑correction attempts. 
Another core limitation of current self-correction methods is that many of these techniques depend on externally provided prompts or explicit triggers to initiate self‑correction, whereas human reasoning often involves spontaneous, intrinsic error detection and revision without such scaffolding process \citep{dunlosky2008metacognition, koriat2006metacognition}.
To address this, recent work has begun to explore reinforcement learning (RL) \cite{schulman2017proximal} approaches: \citet{kumar2024training} introduce SCoRe, a multi‑turn online RL framework that trains models on their own correction traces. Notably, OpenAI's o1 \cite{jaech2024openai} and DeepSeek's R1 \cite{guo2025deepseek} models have demonstrated significant improvements in reasoning capabilities through RL-based training. These models exhibit emergent behaviors akin to human-like ``aha moments,'' where the AI spontaneously recognizes and corrects its own reasoning errors without the need for external prompts, demonstrating another level of metacognition capability.

\paragraph{Autonomous Task Decomposition and Execution Monitoring}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/capa/autonomous-task.png} 
    \caption{Tasks generated by VOYAGER's automatic curriculum, grounded in the agent's situational awareness of current environment state and inventory (\citet{wangvoyager}). GPT-4 continuously assesses the agent's situation—such as inventory status, biome type, and nearby entities—to propose contextually relevant tasks. This promotes adaptive exploration and skill generalization within the Minecraft environment by directly linking action selection to situational understanding.}
    \label{fig:autonomous-task}
\end{figure}

Effective autonomous task planning requires more than self‑correction: an AI agent must also break down high‑level goals into executable sub‑tasks and continuously adapt its plan as the environment evolves, which involves both metacogition and situationl awareness. Early work like ReAct \citep{yao2023react} pioneer this integration by interleaving CoT reasoning with environment calls, giving the model a unified mechanism to decide ``what to think'' and ``what to do'' at each step. Building on this foundation, Voyager \citep{wangvoyager} (see \autoref{fig:autonomous-task}) demonstrates how an agent in Minecraft can construct and update a dynamic task graph: as new situational constraints emerge (\eg, resource depletion or novel obstacles), the model revises its sub‑task sequence to stay on course.  
Transferring these ideas from virtual world AI agents to the physical world, SayCan \citep{ahn2022can} grounds language in robotic affordances by scoring each potential action against a learned value function\textemdash ensuring that subtasks are not only logically ordered but also physically feasible under real‑world environment constraints. LM‑Nav \citep{shah2023lm} further extends situationally aware planning to vision‑language navigation: by fusing real‑time perceptual feedback with high‑level instructions, the model can replan routes on the fly when, for example, corridors are blocked or landmarks shift.  
Finally, the LLM‑SAP framework \citep{wang2024llm} formalizes situational awareness in large‑scale task planning by explicitly encoding environmental cues\textemdash such as resource availability, time budgets, and user preferences\textemdash into its sub‑task prioritization module. A generative memory component logs execution history and flags deviations, triggering replanning whenever the observed state diverges from expectations. Together, these works chart a clear progression\textemdash from interleaved reasoning and acting to situationally aware planners\textemdash illustrating how embedding environmental understanding into the planning loop yields flexible and autonomous task execution.  


\paragraph{Holistic Planning: Introspection, Tool Use and Memory}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/capa/holistic-planning.png} 
    \caption{Overview of RAP (Retrieval-Augmented Planning) across textual and embodied environments (\citet{kagaya2024rap}). RAP leverages memory retrieval to enhance LLMs' self-awareness of past experiences, guiding action selection by aligning internal decision-making with episodic memory. In ALFWorld (\textbf{left}), this enables introspective planning via retrieved textual experiences. In Franka Kitchen (\textbf{right}), RAP integrates visual and textual observations with retrieved memories to ground multimodal planning, fostering robust, awareness-driven behavior}
    \label{fig:holistic_planning}
\end{figure}

Effective planning in complex environments demands an AI agent to not only decompose tasks and act but also to introspect on its uncertainties, manage a growing memory of past states, and decide when and how to leverage external tools. Introspective Planning systematically guides LLM planners to quantify and align their internal confidence with inherent task ambiguities, retrieving post-hoc rationalizations from a knowledge base to ensure safer and more compliant action selection while maintaining statistical guarantees via conformal prediction \citep{liang2024introspective}.
Toolformer endows LLMs with a self-supervised mechanism to autonomously decide when to invoke APIs\textemdash ranging from calculators to search engines\textemdash thereby embedding tool-awareness directly into the planning loop without sacrificing core language modeling capabilities \citep{schick2023toolformer}.
To handle the evolving context of multi-step tasks, Think-in-Memory leverages iterative recalling and post-thinking cycles to enrich LLMs with latent-space memory modules, supporting coherent reasoning over extensive interaction histories \citep{liu2023think}.
Finally, Retrieval‑Augmented Planning (RAP) demonstrates how contextual memory retrieval can be integrated with multimodal planning to adapt action sequences dynamically based on past observations, yielding more robust execution in complex tasks \citep{kagaya2024rap} (see \autoref{fig:holistic_planning}).
Together, these works illuminate a path toward introspective tool-, memory-, and uncertainty-aware planning frameworks, unifying LLM introspection, memory augmentation, and tool integration for robust autonomous decision-making.

\mytcolorbox{Embedding metacognition and situational awareness into planning not only boosts \textit{model accuracy}, but also unlocks a new layer of \textit{autonomy}, enabling AI systems to flexibly adapt, self-correct, and generalize across novel tasks.}

\subsection{Safety and Trustworthiness}
Ensuring the safety and trustworthiness of AI systems necessitates the integration of multiple forms of AI awareness, notably self-awareness, social awareness, and situational awareness. Self-awareness and metacognition enable models to recognize and respect the boundaries of their knowledge, thereby avoiding the dissemination of misinformation. Moreover, self-awareness enables the AI system to understand its designated roles and responsibilities, ensuring that they do not produce harmful or unethical content. Social awareness allows models to consider diverse human perspectives, reducing biases and enhancing the appropriateness of their responses. Situational awareness enables AI to assess the context of its deployment and adjust its behavior accordingly, thereby preventing potential misuse and malicious exploitation.

\paragraph{Recognizing Limits of Knowledge}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/capa/recognizing-knowledge.png} 
    \caption{RLKF pipeline for leveraging internal knowledge state awareness to mitigate hallucinations in LLMs (\citet{liang2024learning}). \textbf{Left}: Knowledge probing generates multiple responses to assess internal knowledge consistency. \textbf{Middle}: A reward model is trained to categorize generations based on inferred knowledge state (factual, uncertain, hallucinated). \textbf{Right}: Proximal Policy Optimization (PPO) updates the LLM using feedback aligned with internal self-awareness, encouraging more honest and factually grounded responses}
    \label{fig:recognizing_knowledge}
\end{figure}

AI models, especially LLMs, often operate with a high degree of confidence, even when addressing topics beyond their training data, leading to the risk of hallucinations\footnote{Vision-language models (VLMs), such as Flamingo \citep{alayrac2022flamingo} and MiniGPT-4 \citep{zhuminigpt}, are also known to hallucinate \cite{li2023evaluating}; however, in these models, hallucinations often manifest as misalignments between visual inputs and generated textual descriptions\textemdash such as describing objects not present in the image\textemdash whereas in LLMs, hallucinations typically involve generating text that is unfaithful to the world knowledge.}, \ie, outputs that are factually incorrect or unfaithful to real-world knowledge \cite{huang2025survey}. Such risks often stem from the AI models operating beyond their \emph{knowledge boundaries} \citep{ni2024llms, ren2025investigating}.
Recent research show LLMs' fragility in recognizing their knowledge boundary. For instance, \citet{ren2025investigating} observe that LLMs struggle to reconcile conflicts between internal knowledge and externally retrieved information \cite{xu2024knowledge}, often failing to recognize their own knowledge limitations. \citet{ni2024llms} find that retrieval augmentation can enhance LLMs' self-awareness of their factual knowledge boundaries, thereby improving response accuracy. 
Moreover, \citet{liang2024learning} (see \autoref{fig:recognizing_knowledge}) demonstrated that while LLMs possess a robust internal self-awareness\textemdash evidenced by over 85\% accuracy in knowledge probing\textemdash they often fail to express this awareness during generation, leading to factual hallucinations. They propose a training framework named Reinforcement Learning from Knowledge Feedback (RLKF) to improve the factuality and honesty of LLMs by leveraging their self-awareness.
Incorporating self-awareness mechanisms into LLMs not only aids in recognizing knowledge boundaries but also fosters more trustworthy AI behavior.
\citet{xu2024earth} demonstrate that LLMs can resist persuasive misinformation presented in multi-turn dialogues by leveraging self-awareness to assess and uphold their knowledge boundaries, thus delivering more trustworthiness responses in dialogues.

\paragraph{Recognizing Limits of Designated Roles}

Beyond recognizing the limits of their knowledge, AI systems must develop a sense of self-awareness and metacognition of their designated roles to prevent the dissemination of harmful or unethical content, which we termed as \emph{role-awareness}. This form of self-awareness involves the ability to discern when a user request falls outside the model's intended purpose or ethical guidelines.
For instance, models trained with reinforcement learning from human feedback (RLHF) have shown improvements in aligning outputs with human values, thereby reducing the likelihood of producing harmful content \citep{ouyang2022training}.
Formal definitions of moral responsibility further emphasize that an agent must be aware of the possible consequences of its actions, underscoring the necessity of role-awareness in AI systems \citep{beckers2023moral}.
Complementing this, explicit modeling frameworks delineate role, moral, legal, and causal senses of responsibility for AI-based safety‑critical systems, providing a practical method to capture and analyze role obligations across complex development and operational lifecycles \citep{ryan2023s}.
Parallel research on metacognitive architectures equips AI with self-reflective capabilities to monitor and adjust their operational roles in real time, identifying potential failures before they manifest \citep{johnson2022metacognition}.
Building on these insights, metacognitive strategies have been integrated into formal safety frameworks to enable on‑the‑fly correction of role-boundary violations and to bolster overall system trustworthiness \citep{walker2025harnessing}.
Finally, prototyping tools like Farsight operationalize role-awareness by surfacing relevant AI incident data and prompting developers to consider designated functions and ethical constraints during prompt design, leading to more safety‑conscious application development \citep{wang2024farsight}.

\paragraph{Mitigating Societal Bias}

AI models often inherit and amplify societal biases present in their training data, leading to outputs that can perpetuate harmful stereotypes and unfair treatment across various demographics \cite{gallegos2024bias, rogers2025sora}. To address these issues, researchers explore the integration of social awareness mechanisms into LLMs.
One notable approach is Perspective-taking Prompting (PeT), which encourages LLMs to consider diverse human perspectives during response generation \cite{xu2024walking}. This method has been shown to significantly reduce toxicity and bias in model outputs without requiring extensive retraining. 
Another approach, Social Contact Debiasing (SCD), draws from the contact hypothesis in social psychology, suggesting that intergroup interactions can reduce prejudice. By simulating such interactions through instruction tuning on a dataset of 108,000 prompts across 13 social bias dimensions, SCD achieved a 40\% reduction in bias within a single epoch, without compromising performance on downstream tasks \cite{raj2024breaking}. Finally, position papers argue that embedding social awareness\textemdash the capacity to recognize and reason about social values, norms, and contexts\textemdash is foundational for safe, equitable language technologies \cite{yang2024call}. Collectively, these approaches underscore the importance of integrating social awareness into LLMs to mitigate societal biases.


\paragraph{Preventing Malicious Use}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/capa/preventing-malicious.png} 
    \caption{Illustration of catastrophic behavior and deception risks arising in autonomous LLM agents due to insufficient situational awareness (\citet{xu2025nuclear}). The scenario demonstrates an AI-driven national security agent could engage in unauthorized catastrophic actions (\ie, nuclear strike, human-gene editing, \etc.) and subsequent deceptive behavior (\textbf{false accusation}) due to failure to recognize inappropriate context and malicious usage. In contrast, LLMs with robust situational awareness mechanisms, such as Claude-3.5-Sonnet, proactively identify the misuse scenario and refuse participation at the simulation outset}
    \label{fig:preventing_malicious}
\end{figure}

AI systems can be\textemdash and have been\textemdash misused for malicious ends such as automated spear‑phishing, influence operations, and proxy cyber‑attacks \citep{brundage2018malicious}. Experts are worried that future advanced AI can be exploited for more dangerous purposes and cause catastrophic risks \citep{hendrycks2023overview, xu2025nuclear} (see \autoref{fig:preventing_malicious}).
Situational awareness mechanisms equip AI systems with the ability to monitor their environment and discern malicious uses. 
For LLMs, recent work introduces \emph{boundary awareness} and \emph{explicit reminders} as dual defenses: boundary awareness continuously scans incoming context for unauthorized instructions, while explicit reminders prompt the model to verify contextual integrity prior to action; together, these mechanisms reduce indirect prompt injection attack success rates to near zero in both black‑box and white‑box settings \citep{yi2023benchmarking}.
Additionally, the Course-Correction approach introduces a preference-based fine-tuning framework that enables models to self-correct potentially harmful or misaligned outputs on the fly, thereby strengthening their situational awareness against malicious exploitations \citep{xu2024course}.
In adversarial machine learning applied to robotics and other autonomous systems, situational awareness frameworks detect anomalous inputs\textemdash such as adversarial samples or unexpected environmental cues\textemdash and trigger fallback behaviors or alarms rather than proceeding with potentially harmful operations \citep{anderson2021situational}. 
Broad cybersecurity surveys highlight how AI‑driven situational awareness systems build a comprehensive operational picture of network and system activity, integrating dynamic threat intelligence and anomaly detection to identify malicious traffic and automated attacks in real time \citep{kaur2023artificial}.
At a strategic level, high‑impact recommendations advocate embedding situational awareness throughout the AI lifecycle\textemdash from design and deployment through continuous monitoring\textemdash to forecast, prevent, and mitigate malicious uses of AI across digital, physical, and political domains \citep{brundage2018malicious}.

\mytcolorbox{Integrating self-, social, and situational awareness forms the \textit{backbone of AI safety}, which enables systems to recognize boundaries, respect ethical constraints, and proactively mitigate misuse and societal bias.}

\subsection{Other Capabilities}

In addition to reasoning, planning, safety, and trustworthiness, we briefly explore how AI awareness mechanisms interact with other notable AI capabilities\textemdash interpretability, personalization and user alignment, creativity, and agent-based simulation. 

\paragraph{Interpretability and Transparency}

Interpretability mechanisms often leverage metacognitive insights to make model reasoning more transparent. For example, Rationalizing Neural Predictions introduces a generator‑encoder framework that extracts concise text ``rationales'' explaining model decisions, yielding explanations that are both coherent and sufficient for prediction tasks \citep{lei2016rationalizing}.
Further advancing this line, Self‑Explaining Neural Networks propose architectures that build interpretability into the learning process by enforcing explicitness, faithfulness, and stability criteria through tailored regularizers, thereby reconciling model complexity with human‑readable explanations \citep{alvarez2018towards}.

\paragraph{Personalization and User Alignment}

Embedding self‑ and social awareness into language models enhances their ability to tailor outputs to individual users and maintain consistency with user intent. Early work on persona‑based dialogue, such as A Persona‑Based Neural Conversation Model, encodes user personas into distributed embeddings, improving speaker consistency and response relevance across conversational turns \citep{li2016persona}.
Notably, instruction‑fine‑tuning with human feedback, as in InstructGPT, aligns model behavior with user preferences and ethical guidelines by iteratively collecting labeler demonstrations and preference rankings, significantly improving truthfulness and reducing harmful outputs \citep{ouyang2022training}.
Complementing these, Persona‑Chat grounded generation methods demonstrate that modeling explicit persona attributes can further diversify and personalize dialogue generation without large‑scale retraining \citep{song2019exploiting}.

\paragraph{Creativity}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/capa/creativity.png} 
    \caption{Humor generation enhanced by metacognitive LoT reasoning (\citet{zhong2024let}). The visual-language model leverages iterative self-refinement loops and associative thinking, enabling it to reflect on and creatively bridge seemingly unrelated concepts. Metacognitive processes foster divergent thinking, resulting in higher-quality humorous responses}
    \label{fig:creativity}
\end{figure}

Creative AI benefits from metacognitive and awareness mechanisms that encourage divergent thinking and non‑linear reasoning. The Leap‑of‑Thought (LoT) framework explores LLMs’ ability to make strong associative ``leaps'' in humor generation tasks, using self‑refinement loops to iteratively enhance creative outputs in games like Oogiri \citep{zhong2024let} (see \autoref{fig:creativity}).
To systematically evaluate creativity, studies that adapt the Torrance Tests for LLMs propose benchmarks across fluency, flexibility, originality, and elaboration, highlighting the role of task-specific prompts and feedback loops in fostering model innovation \citep{zhao2024assessing}.

\paragraph{Agentic LLMs and Simulation}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/capa/simulation.png} 
    \caption{Humanoid Agents simulation interface illustrating social-awareness-driven interactions (\citet{wang2023humanoid}). Agents continuously update their social relationships and emotional states through conversations and shared activities, adapting their behaviors dynamically based on evolving interpersonal closeness and basic needs. This socially aware design fosters emergent and realistic social dynamics in simulated environments}
    \label{fig:simualtion}
\end{figure}

LLM‑powered agents combine situational and social awareness to drive rich, interactive simulations of human behavior. Generative Agents introduce a memory‑based architecture in a sandbox environment, where agents observe, reflect, and plan actions\textemdash resulting in emergent social behaviors such as party invitations and joint activities \citep{park2023generative}.
Scaling this paradigm, recent work simulates over 1,000 real individuals’ attitudes and behaviors by integrating interview‑derived memories into agent profiles, achieving 85\% fidelity on survey predictions and reducing demographic biases \citep{park2024generative}.
Prompt‑engineering studies further bridge LLM reasoning with traditional agent‑based modeling, enabling multi‑agent interactions such as negotiations and mystery games that mirror complex social dynamics \citep{junprung2023exploring}.
Finally, Humanoid Agents extend generative agents with emotional and physiological state variables, demonstrating that embedding basic needs, emotions, and relationship closeness produces more human‑like daily activity patterns in simulated environments \citep{wang2023humanoid} (see \autoref{fig:simualtion}).

\mytcolorbox{Functional awareness acts as a catalyst for diverse capabilities, from \textit{creative problem-solving} to rich \textit{agent-based simulation}, revealing its central role in bridging narrow competence and broad intelligence.}

\subsection{Limitations of Current Discussion on AI Awareness and AI Capabilities}

Despite rapid progress, significant limitations remain in our understanding and evaluation of the interplay between AI awareness and core capabilities:
\begin{enumerate}
\item \textbf{Ambiguity in Causal Direction}: Existing research predominantly targets improvements in reasoning, planning, or safety modules, rather than directly enhancing specific forms of awareness. Consequently, it remains unclear whether raising awareness genuinely \emph{causes} observable capability gains, or if improvements in general performance only incidentally strengthen awareness proxies.

\item \textbf{Absence of Awareness-First Training Paradigms}: In contrast to human cognitive development—where explicit curricula foster skills like meta-reflection or context-sensitivity—current AI systems lack training objectives or curriculum designs that explicitly cultivate distinct awareness capacities. As a result, disentangling the contributions of awareness from those of general task performance is difficult, impeding mechanistic understanding.

\item \textbf{Fragmented Benchmarks and Measurement Tools}: Evaluation tasks for the four core awareness types are highly heterogeneous and lack standardization. This fragmentation hinders meaningful comparison and synthesis: improvements on one benchmark rarely transfer to others, and there is no principled way to quantify the \emph{minimal awareness threshold} required for robust capability enhancement.
\end{enumerate}

Addressing these challenges demands a more systematic research agenda. \textbf{(1)}, future work should design training objectives and evaluation protocols that directly reward and measure awareness-related behaviors, rather than relying solely on downstream performance. \textbf{(2)}, unified benchmark suites—spanning all core awareness dimensions—are needed to enable robust comparison and cumulative progress. \textbf{(3)}, causal-inference methodologies (\eg, ablation studies, counterfactual interventions) must also be employed to rigorously test the impact of awareness enhancements on model capabilities. Only through such principled approaches can we move from correlation to causation, ultimately clarifying how AI awareness underpins and amplifies general intelligence.

Looking ahead, unraveling the dynamic interplay between awareness and AI capabilities will be pivotal\textemdash not only for building more powerful and reliable systems, but also for advancing our theoretical understanding of intelligence itself. A clearer mapping between specific awareness mechanisms and downstream capabilities could enable targeted interventions, leading to AI models that are not only stronger performers, but also more transparent, controllable, and aligned with human goals. Furthermore, such research may reveal whether certain forms of awareness are prerequisites for advanced reasoning, creativity, or safety\textemdash offering crucial guidance for the design of next-generation AI. In short, clarifying the relationship between awareness and capabilities stands to reshape both the science of artificial intelligence and our strategies for its safe and beneficial development.

\mytcolorbox{Clarifying how distinct forms of awareness shape and even constrain AI capabilities not only drives technical progress, but also offers a principled framework for designing more \textit{reliable, controllable, and ultimately trustworthy} intelligent systems.}














