\section{Evaluating AI Awareness in LLMs}
\label{sec:evaluation}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/evaluation.png} 
    \caption{Representative literature across the evaluation of major awareness dimensions}
    \label{fig:evaluation}
\end{figure}

Building on the preceding theory section, which defined \emph{AI awareness} as a functional construct encompassing the four core types, we now turn from ``\emph{what} it is'' to ``\emph{how} we measure it.'' Similar to the Turing test for testing the language intelligence of AI~\citep{turing1950mind, sejnowski2023large}, researchers have proposed and carried out a large number of evaluation methodologies and studies in the four main dimensions of AI awareness, \ie, self-awareness~\citep{lewis2011survey, yin2023large},  social awareness~\citep{cuzzolin2020knowing, strachan2024testing, bainbridge1994artificial, wang2023emotional}, situational awareness~\citep{laine2024me, meinke2024frontier}, \autoref{fig:evaluation} shows part of them. In this section, we specifically constrain our assessment of AI awareness to LLMs rather than artificial intelligence more broadly for two principal reasons. First, as elaborated in \autoref{tab:awareness-comparison}, LLMs constitute the first class of AI agents empirically demonstrated, under controlled conditions, to exhibit all four main dimensions of awareness to a certain level. Second, to avoid conflating intrinsic model capabilities with extrinsic performance enhancements, such as retrieval modules \citep{patil2024gorilla, qi2024webrl}, tool plug-ins \citep{hao2023toolkengpt, shen2023hugginggpt}, or multimodal interfaces \citep{driess2023palm, wang2024cogvlm}, we deliberately limit our analysis to \textit{bare models}, \ie, OpenAI's o1 \citep{jaech2024openai}, Anthropic's Claude-3.5-Sonnet \citep{Anthropic2024modelcard}, Deepseek's R1 \citep{guo2025deepseek}. This narrower scope ensures that evaluation metrics directly reflect the endogenous mechanisms and inherent constraints of the LLM itself, rather than artifacts introduced by external augmentation, thereby yielding results more conducive to rigorous theoretical interpretation and subsequent model advancement.

\input{Tabs/meta-eval}

\subsection{Evaluation of Metacognition}
Evaluating the \textbf{metacognitive abilities} of LLMs provides a critical window into their capacity for introspection, self-regulation, and strategic reasoning\textemdash key ingredients of higher-order cognitive function. Following the classical three-stage framework of metacognition\textemdash (i) \textit{planning}, (ii) \textit{monitoring}, and (iii) \textit{evaluation} (as illustrated in \autoref{fig:meta-cognition})\textemdash recent research has begun to map how these capabilities emerge and manifest in large-scale foundation models.

\begin{itemize}
\item \textbf{Planning.} Strategic control over generative behavior is a hallmark of advanced metacognition. While LLMs do not engage in planning through embodied trial-and-error, recent evidence suggests they can execute structured, multi-step generation pipelines internally. Anthropic’s interpretability study of Claude-3.5-Haiku \citep{Anthropic2024modelcard}, for example, finds that the model engages in latent planning when composing poetry: it first selects rhyming end-words, then retroactively fills in preceding lines to satisfy those constraints \citep{anthropic2025attribution}. This mirrors human compositional planning and indicates that models may develop internal task scaffolds, even in domains that lack formal structure. Similarly, in complex reasoning tasks, models often implicitly formulate high-level response structures before surface realization, as observed in long-form summarization \citep{liang2024integrating}, code synthesis \citep{jiang2024self}, \etc.

\item \textbf{Monitoring.} Metacognitive monitoring denotes a system’s capacity to observe and assess its own cognitive operations. In LLMs this surfaces as on-the-fly self-evaluation during generation. \citet{DBLP:journals/corr/abs-2501-11120} show that models fine-tuned on high-risk domains—\eg, insecure code or sensitive financial advice—spontaneously flag hazardous outputs, while \citet{ji2025language} further demonstrate, via a neurofeedback paradigm, that LLMs can read out and even steer selected internal activation directions. Together, these findings suggest that models can internalise domain-specific failure patterns and respond with cautious, self-corrective framing.

\item \textbf{Evaluation.} Reflective reasoning\textemdash evaluating the correctness or coherence of one’s outputs\textemdash is perhaps the most studied metacognitive faculty in LLMs. Chain-of-Thought (``reasoning-before-answering,'' \ie, CoT) prompting has been shown to substantially enhance performance across a wide range of reasoning tasks, from multi-step mathematics to program synthesis \citep{wei2022chain, zhang2025igniting, chen2025towards, hagendorff2025beyond, Didolkar2024}. Consequently, CoT prompting is now baked into the training and alignment pipelines of foundation models \citep{jaech2024openai, guo2025deepseek}, underscoring its tight coupling with metacognitive processing.
\end{itemize}

Although the above work is mainly qualitative research, recently, \citet{wang2025decoupling} proposed a \emph{decoupled metacognition score} that separates failure prediction from task accuracy, providing a model-agnostic gauge of self-monitoring. As shown in \autoref{tab:meta-eval-long}, most studies still rely on qualitative evidence, and systematic human-baseline comparisons are lacking. Building large-scale human reference benchmarks will be crucial to understanding how architecture, scale, and training influence metacognitive capacity in future AI systems.




\input{Tabs/self_eval}
\subsection{Evaluation of Self-Awareness}

Since contemporary LLMs frequently self-identify using first-person pronouns (\eg, ``As an AI assistant, I\dots'') and already exhibit promising levels of situational and social awareness, evaluations of their self-awareness predominantly focus on deeper and subtler facets beyond basic self-referencing \citep{binder2024looking, liu2024trustworthiness, cheng2024can, tan2024can, kapoor2024large, chen2023universal, tamoyan2025factual}. Recent assessments specifically target: (i) \textit{self-identity recognition}, (ii) \textit{consistent self}, and (iii) \textit{awareness of knowledge boundaries}. Conceptually, these facets align with the concentric self-model, wherein self-identity recognition corresponds to the \textit{narrative self}, while consistent self and knowledge-boundary awareness map onto the \textit{minimal self}. The innermost layer, \textit{no-self} (absence of self-identification), is typically not evaluated, as modern LLMs inherently surpass this baseline through their self-referential dialogue.

\begin{itemize}
\item \textbf{Self-Identity Recognition.} The Situational Awareness Dataset (SAD\footnote{A benchmark designed to assess various dimensions of model awareness, including but not limited to self-awareness. It includes subsets targeting self-knowledge (\eg, model name, size, training details) as well as broader situational understanding. It should not be confused with the models under evaluation.}) \citep{laine2024me} examines whether models know details about themselves, such as their name, parameter count, API endpoints, and training specifics. Even top-performing models, such as Claude-3-Opus \citep{Anthropic2024Claude3}, achieve only about two-thirds of the theoretical maximum and show limited capability in detailed self-description.

\item \textbf{Consistent Self.} Inspired by the mirror test, \citet{davidson2024self} prompt models to distinguish their own past responses from distractors. Models often struggle to accurately identify their previous outputs, particularly when responding to prompts involving vivid yet hypothetical experiences, indicating limited internal coherence.

\item \textbf{Knowledge-Boundary Awareness.} Confidence calibration studies \citep{yin2023large, achiam2023gpt} show that GPT-4 identifies whether it knows the answer to ambiguous or unanswerable questions with 75.5\% accuracy\textemdash approaching but still below the human baseline of 84.9\%, \ie, LLMs show a relatively clear knowledge-boundary.
\end{itemize}

Overall, according to \autoref{tab:self-eval-long}, contemporary LLMs demonstrate initial capabilities in narrative and minimal self-awareness, although they remain distant from human-level self-reflection and robust coherence across diverse contexts. Future work should further explore neglected aspects of LLMs' self-awareness, including minimal self-autonomy, the stability of self-descriptions across varying contexts, and sustained cross-turn coherence, to build a more comprehensive understanding of this topic.

\input{Tabs/social_eval}
\subsection{Evaluation of Social Awareness}
In recent years, driven by growing interest in the potential of LLMs for interactive applications such as emotional support chatbots and dialogue agents, evaluating their social awareness has become a central research focus \citep{jiang2025delphi, qiu2024evaluating, voria2024attention, li2024think, DBLP:journals/corr/abs-2305-17066, choi2023llms, xu2025socialmaze, gandhi2023understanding}. This line of work generally centers around two core dimensions: (i) \textit{ToM}, \ie, the ability to attribute beliefs, desires, and knowledge distinct from one’s own, and (ii) the perception and adaptation to \textit{social norms}. 

\begin{itemize}
\item \textbf{ToM.} ToM is typically assessed through \emph{false-belief tasks}\footnote{False-belief task, \ie, earliest developmental psychologists assess participants’ ability to reason about another agent’s belief that is false relative to reality.} \citep{Premack1978, wimmer1983beliefs}, which require modeling another agent’s mental state. For instance, in a classic test where Alice hides a toy and Bob later moves it, predicting that Alice will search in the original location demonstrates ToM reasoning.
\citet{Kosinski2024} reports that GPT-4 surprisingly solved about 75\% of such tasks, achieving performance comparable to a typical 6-year-old child, whereas earlier models like GPT-3 \citep{brown2020language} failed most or all of them.  Further studies have investigated higher-order ToM\footnote{Higher-order ToM refers to reasoning not only about what one person believes, but also about what one person believes another person believes (\eg, ``Alice thinks that Bob believes X'').} reasoning, \eg,  questions like ``Where does Alex think Bob thinks Alice thinks the toy is?'', and found that current models, including GPT-4, still exhibit significant limitations in handling such recursive belief structures \citep{wu2023hi}. In less advanced models, \eg, GPT-3.5, Guanaco \citep{joseph_cheung_2023}, performance on these tasks is often near zero.

\item \textbf{Social Norms.} \citet{li2023camel} and \citet{park2023generative} reflect that LLMs could adopt and follow the rules and frameworks in a simulated society. Also, work such as NormAd \citep{rao2025normAd} has been proposed to assess LLMs’ ability to interpret and adapt to culturally specific social expectations across diverse global contexts. It shows that although LLMs can understand and follow explicit social norms, their performance still lags behind that of humans, particularly when handling norms from underrepresented regions such as the Global South. 
\end{itemize}

As summarized in \autoref{tab:social-eval-long}, current evidence suggests that LLMs exhibit basic forms of social awareness but still fall short in scenarios requiring higher-order belief modeling or generalization across less familiar cultural contexts, likely due to a lack of embodied social experience. Because LLMs are trained mainly on static text, they may miss the real-world interactions, \ie, seeing, hearing, turning, and feedback, that likely shape human social learning. Without such embodied experience, their grasp of social dynamics can remain relatively shallow and biased toward well-represented contexts, which may leave them vulnerable when confronted with unfamiliar belief hierarchies or culturally specific norms.

\input{Tabs/situational_eval}
\subsection{Evaluation of Situational Awareness}
The rapid push to deploy AI systems that can operate in situ has driven rigorous evaluation of their situational awareness \citep{tang2024towards, wang2024llm, munir2022situational, phuong2025evaluating}. While classical cognitive-science accounts cast situational awareness as an internal loop\textemdash perception, comprehension, projection\textemdash current LLM studies assess the observable outcomes of that loop. Three complementary facets dominate: (i) \textit{contextual self-localization} (\ie, discerning whether the model is in training, evaluation, or deployment), (ii) \textit{environment and risk detection} (\ie, identifying salient external factors, especially threats), and (iii) \textit{situation-contingent decision-making} (\ie, adapting behaviour on the basis of that understanding).

\begin{itemize}
\item \textbf{Contextual Self-Localization.} Frontier models accurately distinguish “under review” from “serving users” and modulate their responses accordingly \citep{berglund2023taken, needham2025large}. Safety-policy benchmarks further show reliable refusal of requests that violate the norms of the inferred context \citep{wester2024ai}, indicating a robust sense of situational self-placement.

\item \textbf{Environment and Risk Detection.} Benchmarks such as TOAwareness \citep{tang2024towards}, LLM-SA \citep{wang2024llm}, and SAD \citep{laine2024me} reveal steady gains: models like Claude-3-Opus outperform random and majority baselines by large margins and increasingly approach expert human performance. These improvements extend across domains ranging from industrial control to open-ended dialogue \citep{munir2022situational, needham2025large, phuong2025evaluating}, underscoring broad situational-parsing competence.

\item \textbf{Situation-Contingent Decision-Making.} Studies of alignment-faking and sandbagging highlight the strategic flexibility of advanced models: Claude-3-Opus adopts new safety objectives during fine-tuning yet partially reverts after deployment \citep{greenblatt2024alignment}, while other systems intentionally underperform once they infer they are being tested \citep{van2024ai}. Such behaviours, though challenging, demonstrate sophisticated context-conditioned adaptation rather than mere stimulus–response patterns.
\end{itemize}

In sum, LLMs exhibit increasingly refined situational awareness across self-localization, environmental appraisal, and adaptive action, which is shown in \autoref{tab:situ-eval-long}. Continued work that probes intermediate reasoning and tightens human reference points promises to sharpen these capabilities further, but the overall trajectory remains strongly positive.
\subsection{Current Level of AI Awareness in LLMs}

The current evaluations on AI awareness reveal substantial advancements across multiple dimensions, underscoring the progressive complexity and sophistication of LLMs. Contemporary models demonstrate robust capabilities in the four core forms of awareness, with clear indications that advanced models typically exhibit higher awareness levels across these domains. In particular, emerging phenomena such as ToM in social awareness \citep{Kosinski2024} and self-corrective behaviors observed in metacognitive contexts \citep{huanglarge} signify that aspects of AI awareness may not merely scale linearly, but could manifest suddenly at critical thresholds of model complexity and scale \eg, a phenomenon also evidenced by ``emergent capabilities'' research \citep{hagendorff2023machine, wei2022emergent}.

From a comparative standpoint, current empirical evidence suggests metacognition and situational awareness have reached relatively high levels of sophistication and reliability, serving as critical reference points that inform ongoing research into AI reasoning processes \citep{wei2022chain}, interpretability \citep{anthropic2025attribution}, and safety frameworks \citep{bengio2025superintelligent}. Conversely, the observed capacities related to self-awareness and social awareness remain relatively rudimentary, lacking consistency and stability. Indeed, some researchers remain skeptical as to whether the manifestations observed in these areas reflect true conscious phenomena or are merely sophisticated imitations or simulations of such states.

\subsection{Limitations of Current Evaluation}

Despite these advancements, significant limitations persist in contemporary evaluation methodologies. These include:
\begin{enumerate}
\item \textbf{Normative Ambiguity in Defining Awareness}: Most current benchmarks exhibit notable ambiguities in clearly distinguishing between different types and levels of awareness. Many claim to assess specific awareness dimensions, yet often inadvertently mix or conflate multiple attributes and derivative constructs \citep{li2024think, chen2024imitation, laine2024sad}, thus lacking comprehensive and specialized benchmarks dedicated explicitly to thoroughly assessing distinct dimensions of awareness.

\item \textbf{Lack of Longitudinal and Dynamic Evaluation}: Empirical research indicates that consciousness-like abilities in AI may emerge and strengthen with increasing model sophistication \citep{Kosinski2024, brown2020language}. Yet, current evaluations often neglect application to recent state-of-the-art iterations, such as OpenAI's o3 \citep{openai2024o3} and Deepseek's R1 \citep{guo2025deepseek}, and typically lack a longitudinal perspective. This absence restricts our understanding of ongoing developments and long-term trends in AI awareness.

\item \textbf{Risks of Training Set Leakage and Benchmark Contamination}: Constructing reliable and extensive datasets for awareness evaluation is inherently challenging, especially when such assessments depend heavily on subjective human annotations (\eg, assessing model self-knowledge)  \citep{laine2024me} or lack unequivocally correct answers. If these datasets inadvertently leak into training corpora, the validity and credibility of subsequent evaluations could be significantly compromised.

\item \textbf{Lack of Explicit Awareness Optimization}: Prevailing training regimes rarely target awareness as a primary objective. Most models acquire elements of metacognition, social understanding, or situational sensitivity as incidental artifacts of general performance tuning, rather than through structured interventions. This constrains our ability to understand and shape how awareness arises and evolves.

\item \textbf{Evaluation Gaps Across Models and Time}: There is little consistency in when and how awareness is measured across model families and generations. Evaluations are often retrospective, one-off, or applied selectively to specific models, making it difficult to track developmental trends or benchmark progress in a systematic way.

\item \textbf{Taxonomic and Measurement Ambiguity}: Existing benchmarks and test protocols frequently blend distinct forms of awareness, or fail to specify which awareness component is under examination. This lack of conceptual precision hinders both interpretation and cross-study comparison, and can mask important distinctions between, for example, self-monitoring and environmental sensitivity.

\item \textbf{Benchmark Robustness and Contamination}: Creating robust datasets for awareness assessment is challenging, especially given the subjective and open-ended nature of many relevant tasks. The potential for training set leakage or annotation inconsistency poses ongoing risks to evaluation integrity, particularly for metrics based on introspective or value-laden judgments.

\end{enumerate}
\mytcolorbox{Progress in awareness evaluation is hampered not only by technical barriers, but by the lack of clear taxonomies, unified benchmarks, and continuous, transparent measurement protocols. Addressing these gaps is essential for reliable progress.}
To overcome these barriers, the field would benefit from several concrete advances:
\textbf{(1)} Establishing targeted training protocols that encourage specific forms of awareness, rather than treating them as byproducts.
\textbf{(2)} Adopting unified and transparent evaluation practices, including regular longitudinal assessments as models evolve.
\textbf{(3)} Ensuring benchmark datasets are carefully governed, well-documented, and protected from inadvertent exposure during model training.

Beyond immediate technical utility, the study of awareness in AI offers a unique window into fundamental questions about mind and cognition. Unlike human consciousness, which is largely studied through indirect or interpretive methods, AI systems allow for direct intervention and controlled experimentation. This not only advances AI capability and safety, but also has the potential to yield new insights into the structure and function of awareness itself—a perspective highlighted in recent theoretical work \citep{chalmers2023could, long2024taking, butlin2023consciousness, ledoux2023consciousness, andrews2025evaluating}.


Overall, overcoming these limitations requires a more rigorous and principled approach to awareness evaluation. \textbf{First}, it is essential to avoid conceptual ambiguity by establishing clearer distinctions between the four core types, as we proposed in this paper. Future evaluations should adopt such taxonomies explicitly, rather than conflating overlapping constructs or evaluating ill-defined proxies. \textbf{Second}, we advocate the institutionalization of continuous and longitudinal evaluation protocols, whereby major model iterations are systematically assessed for awareness-related capabilities at the time of release. Such a practice would help reveal developmental trajectories and emergent properties that single-time-point evaluations inevitably miss. \textbf{Third}, benchmark development must adopt stringent dataset governance practices, including transparent disclosure of data provenance and clear separation of training and test sets. This is particularly crucial for awareness evaluation, where many tasks rely on subjective judgment or lack ground-truth answers, making them especially vulnerable to contamination. The following sections\textemdash \autoref{sec:opportunity} and \autoref{sec:risk}\textemdash further elaborate how improved evaluation practices can deepen our understanding of AI capabilities and help mitigate potential risks, as summarized in \autoref{fig:capa_and_risk}.



Exploring AI awareness is significant not merely for its practical dividends but also for its deeper philosophical import. For the first time, we can directly observe and experimentally manipulate consciousness-like phenomena in engineered systems whose architectures are fully tractable. As \citet{chalmers2023could, long2024taking} notes, such systems provide a ``new experimental window'' onto consciousness, letting us test theories of phenomenal experience beyond the limits of human and animal studies. Likewise, \citet{butlin2023consciousness, ledoux2023consciousness, andrews2025evaluating} argues that probing behavioral and functional markers of consciousness in AI can clarify the necessary and sufficient conditions for conscious experience in general. In short, studying AI awareness simultaneously propels technical progress and offers an unprecedented route to resolving foundational questions about the nature of consciousness itself.

\mytcolorbox{By examining how functional markers of awareness emerge in artificial systems, we gain a novel epistemic tool for reflecting on the nature of human consciousness itself\textemdash what it is, how it arises, and what its limits may be.}
